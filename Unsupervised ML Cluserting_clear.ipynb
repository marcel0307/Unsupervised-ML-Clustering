{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c548c0-1f50-433f-8c11-3e0e8585e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0d94d",
   "metadata": {},
   "source": [
    "### Exploretory Analysis\n",
    "### 1. General Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_sas(\"a2z_insurance.sas7bdat\")\n",
    "file.set_index(\"CustID\", inplace=True)\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5111c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file[~file.BirthYear.isnull()][\"BirthYear\"].sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file[~file.FirstPolYear.isnull()][\"FirstPolYear\"].sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc66ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.info();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d6815",
   "metadata": {},
   "source": [
    "### 2. Seperation in Metric & NonMetric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb016e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = [\"FirstPolYear\",\"BirthYear\",\"MonthSal\",\"CustMonVal\",\"ClaimsRate\",\"PremMotor\",\"PremHousehold\",\n",
    "               \"PremHealth\",\"PremLife\",\"PremWork\"]\n",
    "nonmetric_cols = [\"EducDeg\",\"GeoLivArea\",\"Children\"]\n",
    "\n",
    "\n",
    "#Assign Datatypes -> EducDeg Categorical logical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ce1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "# All Numeric Variables' Box Plots in one figure\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_cols) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each box plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), metric_cols): # Notice the zip() function and flatten() method\n",
    "    sns.violinplot(x=file[feat], ax=ax)\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \"Numeric Features' Violin Plots\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1bd65",
   "metadata": {},
   "source": [
    "### 3. Unique value count for NonMetric Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "NonMetricFile = file[nonmetric_cols]\n",
    "Countdict = dict()\n",
    "for col in NonMetricFile.columns:\n",
    "    Countdict[str(col)] = len(NonMetricFile[str(col)].dropna().unique())\n",
    "lel = pd.DataFrame.from_dict(Countdict, orient='index', columns= [\"Count\"])\n",
    "lel\n",
    "ax = lel.plot(kind = \"bar\",ylabel='count',\n",
    "         xlabel='Degree', figsize=(6, 5), colormap='Paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb904cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = file[metric_cols]\n",
    "scaler = MinMaxScaler()\n",
    "data[metric_cols] = scaler.fit_transform(data)\n",
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f93d7e",
   "metadata": {},
   "source": [
    "### 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e99ae9",
   "metadata": {},
   "source": [
    "#### I. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33a8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"Missing_Values\"] = file.isnull().sum(axis=1).to_list()\n",
    "sefs = file.groupby(\"Missing_Values\")[[\"Missing_Values\"]].count().rename(columns={\"Missing_Values\":'Missing_Values_count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"Missing_Values\"] = file.isnull().sum(axis=1).to_list()\n",
    "sefs = file.groupby(\"Missing_Values\")[[\"Missing_Values\"]].count().rename(columns={\"Missing_Values\":'Missing_Values_count'}).reset_index()\n",
    "sefs1 = sefs[sefs.Missing_Values > 0]\n",
    "plt.bar(\"Missing_Values\",\"Missing_Values_count\",data = sefs1)\n",
    "plt.xlabel(\"Amount_Missing_Values\")\n",
    "xlocs = plt.xticks()\n",
    "xlocs = [x for x in range(1,len(sefs1)+1)]\n",
    "plt.xticks(xlocs)\n",
    "for i, v in enumerate(sefs1.Missing_Values_count.to_list()):\n",
    "    plt.text(xlocs[i]- 0.1, v+4, str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5f39a",
   "metadata": {},
   "source": [
    "#### II. Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file[file.duplicated(keep=False)==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae30c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.sort_values([\"CustMonVal\",\"ClaimsRate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d804c",
   "metadata": {},
   "source": [
    "### 5. Relation between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "corr = file.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True, annot=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")\n",
    "ax.set_yticklabels(\n",
    "    ax.get_yticklabels(),\n",
    "    rotation=45,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c698c4-7fc4-46d0-a0d0-e75a401ce849",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd39b8",
   "metadata": {},
   "source": [
    "### Reconstructering FirstPolYear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30617e41",
   "metadata": {},
   "source": [
    "##### Aquisitioncost = 25?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d4e49-a51f-4005-a34b-8bdc85270336",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test12 = file.copy()\n",
    "test12[test12.Missing_Values >= test12.Missing_Values.max()]\n",
    "Aquisitioncost = test12[test12.Missing_Values >= test12.Missing_Values.max()][\"CustMonVal\"].iloc[0]\n",
    "print(f\"Aquisitioncost: {Aquisitioncost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469efbf",
   "metadata": {},
   "source": [
    "##### Reconstructering Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test12 = test12[test12.ClaimsRate == 0]\n",
    "test12[\"SumPrem\"] = test12.PremMotor+test12.PremHousehold+test12.PremHealth+test12.PremLife+test12.PremWork\n",
    "\n",
    "test12[\"NewFirstPolYear\"] = (test12.CustMonVal-25) / ((test12.SumPrem) - (test12.ClaimsRate * test12.SumPrem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c44a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols.remove('FirstPolYear')\n",
    "file.drop(columns = \"FirstPolYear\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475c0d2",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plausibilitycheck 3.2.1 in Report \n",
    "# here we know that these are two observations that having unplausible values, therefor we set them to nan and impute them later\n",
    "#file['FirstPolYear'] = np.where(file['FirstPolYear'] > 2016, np.NaN, file['FirstPolYear'])\n",
    "file['BirthYear'] = np.where(file['BirthYear'] < 1900, np.NaN, file['BirthYear'])\n",
    "prepro_df = file.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f6424",
   "metadata": {},
   "source": [
    "#### Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b74220-cdd0-4fd4-bd3e-717440691a0b",
   "metadata": {},
   "source": [
    "Univariate Outlier Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e0d8f-b9c0-4892-b06c-a08e17981d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier_IQR(df,factor=1.5):\n",
    "    df_final=df.copy()\n",
    "    Q1 = df_final.quantile(0.25)\n",
    "    Q3 = df_final.quantile(0.75)\n",
    "    IQR = Q3 - Q1 #Every data point between Q1 and Q3 are inside the interquartile range.\n",
    "    return df_final[~((df_final < (Q1 - 1.5 * IQR)) | (df_final > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "\n",
    "removed_iqr = remove_outlier_IQR(file,factor=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2324d08-26dd-45d2-a8ff-a5150eef7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Made by Adriana:\n",
    "\n",
    "def remove_outliers_zscore(df,metric_ft):\n",
    "    \n",
    "    #fill nan with median of column\n",
    "    data = df.copy()\n",
    "    \n",
    "    for col in metric_ft:\n",
    "            data[col].fillna(data[col].median(),inplace=True)\n",
    "    \n",
    "    #Normalize the metric features of the data so that differetn scales dont influence the distance metrics that DBScan uses\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler() #Normalize data with min max scaler\n",
    "    df_normalize = scaler.fit_transform(data[metric_ft])\n",
    "\n",
    "   #fit zscore to the data\n",
    "    for col in data[metric_ft]:\n",
    "        col_zscore = col + '_zscore'\n",
    "        data['zscore'] = (data[col]-data[col].mean())/data[col].std(ddof=0)\n",
    "    \n",
    "    # clean datafame, without outliers\n",
    "    data_outliers = data[abs(data['zscore']>3)]\n",
    "    df_final = pd.concat([data, data_outliers, data_outliers]).drop_duplicates(keep=False)\n",
    "  \n",
    "    return df_final\n",
    "\n",
    "removedzScore = remove_outliers_zscore(file,metric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0e614-6e77-497e-8ce3-4c2d2a948e7f",
   "metadata": {},
   "source": [
    "Multivariate Outlier Detecion Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572cfbd-2752-476a-92ec-607333b111b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_DBScan(df,metric_ft,nonmetric_columns,eps=0.027,mi_sample = 5):\n",
    "    \n",
    "    #fill nan with median of column\n",
    "    data = df.copy()\n",
    "    \n",
    "    for col in metric_ft:\n",
    "            data[col].fillna(data[col].median(),inplace=True)\n",
    "    \n",
    "    #Normalize the metric features of the data so that differetn scales dont influence the distance metrics that DBScan uses\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler() #Normalize data with min max scaler\n",
    "    df_normalize = scaler.fit_transform(data[metric_ft])\n",
    "\n",
    "   #fit DBScan to the data\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    outlier_detection = DBSCAN(\n",
    "      eps = eps,\n",
    "      metric=\"euclidean\",\n",
    "      min_samples = mi_sample,\n",
    "      n_jobs = -1)\n",
    "    clusters = outlier_detection.fit_predict(df_normalize)\n",
    "    \n",
    "    unique, counts = np.unique(clusters, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    data[\"cluster\"] = clusters\n",
    "    return df[data[\"cluster\"]!= -1], dict(zip(unique, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3845ed-e676-4f80-af48-2658534cfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Tune epsilon hyperparameter for DBScan outlier detection\n",
    "\n",
    "# outliernr=list()\n",
    "# epsl=list()\n",
    "# clusternr=list()\n",
    "\n",
    "# #try epsilon values between 0.001 and 0.3\n",
    "# for epsil in tqdm(np.arange(0.001, 0.3, 0.001)):\n",
    "#     _,dicti = remove_outliers_DBScan(file,metric_cols,nonmetric_cols,epsil,20)\n",
    "    \n",
    "#     outliernr.append(dicti[-1])\n",
    "#     epsl.append(epsil)\n",
    "#     if len(dicti.keys()) > 2: #Check if there is more than 1 cluster by checking the number of keys in the dictionary.\n",
    "#         clusternr.append(\">2\")\n",
    "#     else:\n",
    "#         clusternr.append(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4cf47-2694-4ccf-bb3e-60e53db4a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(x=epsl[11:-120],y=outliernr[11:-120],hue=clusternr[11:-120],edgecolor=\"none\")\n",
    "# plt.title(\"#Detected Outliers for different epsilons\")\n",
    "# plt.xlabel(\"Epsilon\")\n",
    "# plt.ylabel(\"Number of detected Outliers\")\n",
    "# plt.xticks([epsl[31],epsl[-120],epsl[-120]/2])\n",
    "# plt.legend(title='# Clusters')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830515b-328e-4134-9a88-5d394416fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code partly taken from https://towardsdatascience.com/multivariate-outlier-detection-in-python-e946cfc843b3\n",
    "\n",
    "def mahalanobis_outlier(df,metric_ft,threshold= 0.97):\n",
    "    data = df[metric_ft].copy()\n",
    "    \n",
    "    for col in metric_ft:\n",
    "        data[col].fillna(data[col].median(), inplace = True)\n",
    "\n",
    "    indexd = data.index\n",
    "    \n",
    "    data = data.to_numpy()\n",
    "\n",
    "    covariance  = np.cov(data , rowvar=False)\n",
    "\n",
    "    # Covariance matrix power of -1\n",
    "    covariance_pm1 = np.linalg.matrix_power(covariance, -1)\n",
    "\n",
    "    # Center point\n",
    "    centerpoint = np.mean(data , axis=0)\n",
    "\n",
    "\n",
    "    from scipy.stats import chi2\n",
    "    # Distances between center point and \n",
    "    distances = []\n",
    "    for i, val in enumerate(data):\n",
    "          p1 = val\n",
    "          p2 = centerpoint\n",
    "          distance = (p1-p2).T.dot(covariance_pm1).dot(p1-p2)\n",
    "          distances.append(distance)\n",
    "    distances = np.array(distances)\n",
    "\n",
    "    # Cutoff (threshold) value from Chi-Sqaure Distribution for detecting outliers \n",
    "    cutoff = chi2.ppf(threshold, data.shape[1])\n",
    "\n",
    "    # Index of outliers\n",
    "    outlierIndexes = np.where(distances > cutoff)\n",
    "    \n",
    "    pdD = pd.DataFrame(data,columns=metric_ft,index = indexd)\n",
    "    pdD.loc[:,\"distances\"]=distances\n",
    "    pdD.loc[:,\"cutoff\"]=cutoff\n",
    "    \n",
    "    pdD = pdD.query(\"distances < cutoff\").drop([\"distances\",\"cutoff\"],axis=1) #Select only rows in which the distance is smaller than the cutoff threshold\n",
    "    df = df[df.index.isin(pdD.index.tolist())]\n",
    "    return df\n",
    "\n",
    "removed_mahalanobis = mahalanobis_outlier(file,metric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def UMAD_Visualization(df,metric_features, cluster_col):\n",
    "    import umap.umap_ as umap\n",
    "    import plotly.express as px\n",
    "    # This is step can be quite time consuming\n",
    "    UMAP1 = umap.UMAP(n_components=3, init='random', random_state=38).fit_transform(df[metric_features])\n",
    "    some_df = df[[str(cluster_col)]].astype(int)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        UMAP1, x=0, y=1, z=2,\n",
    "        color=some_df[str(cluster_col)],width=900,\n",
    "        height=900,labels={'color': 'cluster'}\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3560504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can vizualize 3 dimensions at max. Hence we reduce the dimensionality of the dataset down to 3 by using PCA, that we will descibe in more detail later on.\n",
    "#Although we cannot capture the entire variance with this plot, but a significat amount is caputred by PC1,PC2,PC3\n",
    "\n",
    "def vizualize_outlier_detect_UMAP(df_before, df_after):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    import umap.umap_ as umap\n",
    "    import plotly.express as px\n",
    "    #pca = PCA(n_components=3)\n",
    "    UMAP1 = umap.UMAP(n_components=3, init='random', random_state=38)\n",
    "    df= df_before.copy()\n",
    "    for col in metric_cols:\n",
    "        df[col].fillna(df[col].median(), inplace = True)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    standard_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    df_=pd.DataFrame(UMAP1.fit_transform(standard_scaled_df),columns=[\"D1\",\"D2\",\"D3\"],index=df[metric_cols].index)\n",
    "\n",
    "    df_.loc[df_after.index,\"Outlier?\"]=\"No\"\n",
    "    df_.loc[[el for el in df_before.index if el not in df_after.index],\"Outlier?\"]=\"Yes\"\n",
    "\n",
    "    import plotly.express as px\n",
    "    fig = px.scatter_3d(df_, x='D1', y='D2', z='D3',\n",
    "                  color='Outlier?',title=\"Visualization of Outliers using 3 Principle Components\",width=900,\n",
    "        height=900)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e0a73-8005-4ee2-87cf-6df4cd267555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can vizualize 3 dimensions at max. Hence we reduce the dimensionality of the dataset down to 3 by using PCA, that we will descibe in more detail later on.\n",
    "#Although we cannot capture the entire variance with this plot, but a significat amount is caputred by PC1,PC2,PC3\n",
    "\n",
    "def vizualize_outlier_detect(df_before, df_after):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    df= df_before.copy()\n",
    "    for col in metric_cols:\n",
    "        df[col].fillna(df[col].median(), inplace = True)\n",
    "    \n",
    "    #PCA requires the df to be scaled.\n",
    "    standard_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    df_pca=pd.DataFrame(pca.fit_transform(standard_scaled_df),columns=[\"PC1\",\"PC2\",\"PC3\"],index=df[metric_cols].index)\n",
    "\n",
    "    \n",
    "    #Add a columns that says if a data point has been recognized as an outlier\n",
    "    df_pca.loc[df_after.index,\"Outlier?\"]=\"No\"\n",
    "    df_pca.loc[[el for el in df_before.index if el not in df_after.index],\"Outlier?\"]=\"Yes\"\n",
    "\n",
    "    #Use plotly to make a 3d scatterplot \n",
    "    import plotly.express as px\n",
    "    fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3',\n",
    "                  color='Outlier?',title=\"Visualization of Outliers using 3 Principle Components\")\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629c766-a7b0-4d39-858f-654cc0ebea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Isolation Forest\n",
    "def IsolationTreez3(df,metric_columns, contamination,*argv):\n",
    "    ''' This function will identify outlier by using all columns that are at least ordinal scaled.\n",
    "        Inputs: DataFrame, metric columns, contamination = Amount of expected Outliers and all additional ordinal columns'''\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    to_label = list()\n",
    "    for arg in argv:\n",
    "        to_label.append(arg)\n",
    "\n",
    "    data = df[metric_columns].copy()\n",
    "    for col in data.columns:\n",
    "        data[col].fillna(data[col].median(),inplace=True)\n",
    "    for col in to_label:\n",
    "        data[col] = df[col]\n",
    "        data[col].fillna(data[col].mode().iloc[0])\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(data[col])            \n",
    "        data[col] = integer_encoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = IsolationForest(n_estimators=200,max_samples=0.9,bootstrap = True, contamination=float(contamination),random_state=np.random.RandomState(49))\n",
    "    model.fit(data.values)\n",
    "    data[\"iforest\"] = model.predict(data.values).tolist()\n",
    "    print(data[\"iforest\"].value_counts())\n",
    "    \n",
    "    \n",
    "    df = df[~df.index.isin(data[data[\"iforest\"]==-1].index.tolist())]\n",
    "    return df.copy(),data[data[\"iforest\"]==-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0f1a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "removedIsolationForest,outlier = IsolationTreez3(prepro_df,metric_cols, 0.01)\n",
    "removedDBscan,_ = remove_outliers_DBScan(prepro_df,metric_cols,nonmetric_cols,0.032,20) #0.032 was the result of hyperparametertuning\n",
    "vizualize_outlier_detect(prepro_df,removedDBscan)\n",
    "vizualize_outlier_detect(prepro_df,removedIsolationForest)\n",
    "vizualize_outlier_detect(prepro_df,removed_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b852fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vizualize_outlier_detect_UMAP(prepro_df,removedIsolationForest)\n",
    "vizualize_outlier_detect_UMAP(prepro_df,removedDBscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488eabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualize_outlier_detect(removedIsolationForest,removedIsolationForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afff47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualize_outlier_detect(removedDBscan,removedDBscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fde628-0e75-4937-b361-c59b7d679d87",
   "metadata": {},
   "source": [
    "### Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0a0ab-84b2-42ca-a2c6-667222d4fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "misval_df = removedIsolationForest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  as we explained in 3.3 of our report, here we are replacing the nans the Premiumns of all rows with more than 4 missing Values\n",
    "#  with 0\n",
    "misval_df.loc[misval_df.Missing_Values == 4] = misval_df.loc[misval_df.Missing_Values == 4].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nr of Rows with Missing Values after outlierremoval\n",
    "len(misval_df[misval_df.Missing_Values >0].index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000de4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "misval_df.drop(\"Missing_Values\",1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182697f-467f-410e-bd7b-6d8f54b242b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Imputation of missing values of ht entire dataFrame with mode fpr nonmetric columns und median for metric columns\n",
    "\n",
    "def simple_imputation(df,non_metric):\n",
    "    data = df.copy()\n",
    "    for col in df.columns:\n",
    "        if col in non_metric:\n",
    "            data[col].fillna(data[col].mode()[0], inplace = True)\n",
    "        else:\n",
    "            data[col].fillna(data[col].median(), inplace = True)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f27aa4-90ff-4203-85af-53acc65c765c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def supervised_impute(df_without_outliers): \n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    df_without_outliers[\"EducDeg\"]= pd.Series(preprocessing.LabelEncoder().fit_transform(df_without_outliers.EducDeg)) #Label Encode Educ Degree\n",
    "\n",
    "    discrete = [\"EducDeg\",\"Children\",\"GeoLivArea\"]\n",
    "    continous = [a for a in df_without_outliers.columns if a not in discrete]\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler #Scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df_without_outliers[continous]),columns=continous,index=df_without_outliers[continous].index) #Bring all features to the same scale\n",
    "    df_without_outliers = pd.concat([scaled_df, df_without_outliers[discrete]], axis=1)\n",
    "\n",
    "    summary = dict()\n",
    "\n",
    "    #Hard coded the best ccp_alphas for the decisiontree regressor to make this cell run faster. Please find the code that leads to these optimized alphas in the decision tree section\n",
    "    best_ccp_alphas = {\"BirthYear\":2.2379104047158256e-05,\"MonthSal\":1.0322055686659169e-05,\"PremMotor\":1.5401288299580906e-06,\"PremHealth\":1.5401288299580906e-06,\"PremLife\":1.099183996339592e-05,\"PremWork\":6.7845242778510876e-06,\"EducDeg\":0.0004856187200293349,\"Children\":0.0005468937947268604,\"GeoLivArea\":0.0005179895310502686}\n",
    "    \n",
    "    for colu in tqdm(df_without_outliers.columns):\n",
    "\n",
    "        if df_without_outliers[colu].isnull().sum() == 0: #We can skip a feature in case it has no missing values to impute\n",
    "            continue\n",
    "\n",
    "        simputed = simple_imputation(df_without_outliers,nonmetric_cols)\n",
    "        simputed.loc[:,colu] = df_without_outliers[colu] #Copy back in the target columns with\n",
    "\n",
    "        simputed_df_final = simputed[simputed[colu].notna()].copy() # drop all NaN rows, because those one we neither use for testing nor for training purposes\n",
    "\n",
    "        X = simputed_df_final.drop(columns = colu)\n",
    "        y = simputed_df_final.pop(colu)\n",
    "\n",
    "        models = dict() #Store different models and their scores in dict\n",
    "\n",
    "        if colu in continous: #Perform various Regression models on continous columns\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 15)\n",
    "\n",
    "\n",
    "\n",
    "            #Linear Regression\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "\n",
    "            model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "            from sklearn.metrics import explained_variance_score\n",
    "            from sklearn.metrics import mean_squared_error\n",
    "\n",
    "            models[model] = mean_squared_error(y_test.values, predictions)\n",
    "\n",
    "            #-------------\n",
    "\n",
    "\n",
    "            #KNN:\n",
    "            from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "            kn = dict()\n",
    "            for k in range(2,40): # Tune k as a hyperparameter\n",
    "\n",
    "                knn = KNeighborsRegressor(n_neighbors=k).fit(X_train, y_train)\n",
    "                predictions = knn.predict(X_test)\n",
    "                kn[knn] = mean_squared_error(y_test.values, predictions)\n",
    "\n",
    "            models[min(kn, key=kn.get)] = min(kn.values())\n",
    "\n",
    "            #-------------\n",
    "\n",
    "            #Decision Tree...\n",
    "            from sklearn.tree import DecisionTreeRegressor\n",
    "            \n",
    "            \n",
    "            #deci = DecisionTreeRegressor(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "            #path = deci.cost_complexity_pruning_path(X_train, y_train)\n",
    "            #ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "            #errorss=dict()\n",
    "            #for alp in tqdm(ccp_alphas[::2]):\n",
    "                #deci = DecisionTreeRegressor(random_state=0,ccp_alpha=alp).fit(X_train, y_train)\n",
    "                #predictions = deci.predict(X_test)\n",
    "                #errorss[alp]=mean_squared_error(y_test.values, predictions)\n",
    "\n",
    "\n",
    "\n",
    "            #deci = DecisionTreeRegressor(random_state=0,ccp_alpha=min(errorss, key=errorss.get)).fit(X_train, y_train)\n",
    "            deci = DecisionTreeRegressor(random_state=0,ccp_alpha=best_ccp_alphas[colu]).fit(X_train, y_train)\n",
    "\n",
    "            predictions = deci.predict(X_test)\n",
    "            models[deci] = mean_squared_error(y_test.values, predictions)\n",
    "\n",
    "\n",
    "            #---------------\n",
    "            \n",
    "\n",
    "            simpl = np.full(y_test.shape, int(y_train.median()))\n",
    "            models[\"simple\"]=mean_squared_error(y_test.values, simpl)\n",
    "\n",
    "            best_model = min(models, key=models.get)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif colu in discrete: #Perform various classification models on discrete features     \n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 18,stratify=y) #Use stratify to make sure the same proportion of dicrete values are in the train  and test set.\n",
    "\n",
    "            from sklearn.metrics import accuracy_score\n",
    "\n",
    "            model = LogisticRegression(max_iter=2000).fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            models[model] = accuracy_score(y_test.values, predictions)\n",
    "\n",
    "            #------\n",
    "\n",
    "        # Here I pruned the decision trees using ccp_alpha from the cost complexity path\n",
    "            #deci = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "            #path = deci.cost_complexity_pruning_path(X_train, y_train)\n",
    "            #ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "            #scores=dict()\n",
    "            #for alp in tqdm(ccp_alphas[::2]):\n",
    "                #deci = DecisionTreeClassifier(random_state=0,ccp_alpha=alp).fit(X_train, y_train)\n",
    "                #predictions = deci.predict(X_test)\n",
    "                #scores[alp]=accuracy_score(y_test.values, predictions)\n",
    "\n",
    "\n",
    "            #deci = DecisionTreeClassifier(random_state=0,ccp_alpha=max(scores, key=scores.get)).fit(X_train, y_train)\n",
    "\n",
    "            #models[deci] = max(scores.values())\n",
    "            \n",
    "            deci = DecisionTreeClassifier(random_state=20,ccp_alpha=best_ccp_alphas[colu]).fit(X_train, y_train)\n",
    "            predictions = deci.predict(X_test)\n",
    "            models[deci] = accuracy_score(y_test.values, predictions)\n",
    "\n",
    "            simpl = np.full(y_test.shape, int(y_train.median()))\n",
    "            models[\"simple\"]=accuracy_score(y_test.values, simpl)\n",
    "            best_model = max(models, key=models.get)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(colu,\"Error: colu is neither in discrete nor in continous list\")\n",
    "            continue \n",
    "\n",
    "        print(colu,models)\n",
    "\n",
    "\n",
    "\n",
    "        #final impute\n",
    "\n",
    "\n",
    "        if best_model == \"simple\": #Check if simple median imputation is the best model\n",
    "            print(\"Simple Imputation is better than the other models.\")\n",
    "            \n",
    "        nan_pred = best_model.predict(simputed[simputed[colu].isnull()].drop(columns = colu))\n",
    "        df_without_outliers.loc[df_without_outliers[colu].isnull(),colu] = nan_pred #impute the predictions\n",
    "        \n",
    "        \n",
    "        summary[colu] = models\n",
    "    #df_wo_outlier = pd.DataFrame(scaler.inverse_transform(df_wo_outlier),columns = df_wo_outlier.columns,index=df_wo_outlier.index) #bring all features back to the original scale\n",
    "    rescaled_df = pd.DataFrame(scaler.inverse_transform(df_without_outliers[continous]),columns=continous,index=df_without_outliers[continous].index)\n",
    "    df_without_outliers = pd.concat([rescaled_df, df_without_outliers[discrete]], axis=1)\n",
    "    \n",
    "    suma = pd.DataFrame(summary).reset_index()\n",
    "    suma[\"index\"]=suma[\"index\"].astype(\"str\")\n",
    "    suma[\"index\"]=suma[\"index\"].str[:16]\n",
    "    suma = suma.groupby(\"index\").sum()\n",
    "    suma.replace(0, np.nan, inplace=True)\n",
    "    \n",
    "    x = np.arange(len(nonmetric_cols))\n",
    "    y1 = suma.loc[\"DecisionTreeClas\",nonmetric_cols].dropna()\n",
    "    y2 = suma.loc[\"LogisticRegressi\",nonmetric_cols].dropna()\n",
    "    y3 = suma.loc[\"simple\",nonmetric_cols].dropna()\n",
    "\n",
    "    width = 0.2\n",
    "\n",
    "    # plot data in grouped manner of bar type\n",
    "    plt.bar(x-0.2, y1, width, color='cyan')\n",
    "    plt.bar(x, y2, width, color='orange')\n",
    "    plt.bar(x+0.2, y3, width, color='green')\n",
    "    plt.xticks(x, ['EducDeg', 'Children', 'GeoLivArea'])\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend([\"Decision Tree\", \"Logistic Regression\", \"Median Imputation\"])\n",
    "    plt.show()\n",
    "    \n",
    "    metric = [\"BirthYear\",\"MonthSal\",\"PremMotor\",\"PremHealth\",\"PremLife\",\"PremWork\"]\n",
    "\n",
    "    x = np.arange(suma.shape[1]-len(nonmetric_cols))\n",
    "    y1 = suma.loc[\"KNeighborsRegres\",metric].dropna()\n",
    "    y2 = suma.loc[\"LinearRegression\",metric].dropna()\n",
    "    y3 = suma.loc[\"DecisionTreeRegr\",metric].dropna()\n",
    "\n",
    "    width = 0.2\n",
    "\n",
    "    # plot data in grouped manner of bar type\n",
    "    plt.bar(x-0.2, y1, width, color='cyan')\n",
    "    plt.bar(x, y2, width, color='orange')\n",
    "    plt.bar(x+0.2, y3, width, color='violet')\n",
    "\n",
    "    plt.xticks(x, metric)\n",
    "    plt.ylabel(\"Squared Error\")\n",
    "    plt.legend([\"KNNRegressor\", \"Linear\", \"DecisionTree\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    best_met = pd.DataFrame(summary)[metric].idxmin()\n",
    "    best_non_met = pd.DataFrame(summary)[nonmetric_cols].idxmax()\n",
    "\n",
    "    best_models_for_each_feature=pd.concat([best_met, best_non_met], axis=0)\n",
    "    imputed_df = df_without_outliers.copy()\n",
    "    \n",
    "    return imputed_df,best_models_for_each_feature\n",
    "\n",
    "df_final_imputed,best_m = supervised_impute(misval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364466ac-c03a-46b7-a521-e23ec5dab22d",
   "metadata": {},
   "source": [
    "### Prinicpal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23014cfe-97c0-4ea6-b08e-2d25729a6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() #cause most of our features follow approx. a normal distribution the most suitable scaler is the standardscaler\n",
    "standard_scaled_df = pd.DataFrame(scaler.fit_transform(df_final_imputed[metric_cols]),columns=metric_cols,index=df_final_imputed[metric_cols].index)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(standard_scaled_df)\n",
    "expl_varian = pca.explained_variance_ratio_ #Check how much variance is captured by each Principle component\n",
    "\n",
    "cum_expl_varian = expl_varian.cumsum()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.bar(range(1,10), expl_varian, alpha=0.5, align='center',\n",
    "        label='individual explained variance')\n",
    "plt.xticks(range(1,10))\n",
    "plt.step(range(1,10), cum_expl_varian, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47862483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output PCA table\n",
    "a_pca_df = pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Explained_Variance\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")\n",
    "a_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optNr_comp = a_pca_df[(a_pca_df.Explained_Variance < 0.1) & (a_pca_df.Cumulative > 0.7)].index[0]-1\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_feat = pca.fit_transform(df_final_imputed[metric_cols])\n",
    "pca_feat_names = [f\"PC{i+1}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=df_final_imputed.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2accdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_outlier_detect(df_before, df_after):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    df= df_before[metric_cols].copy()\n",
    "    for col in metric_cols:\n",
    "        df[col].fillna(df[col].median(), inplace = True)\n",
    "    #standard_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    df_pca=pd.DataFrame(pca.fit_transform(df),columns=[\"PC1\",\"PC2\",\"PC3\"],index=df[metric_cols].index)\n",
    "\n",
    "    df_pca.loc[df_after.index,\"Outlier?\"]=\"No\"\n",
    "    df_pca.loc[[el for el in df_before.index if el not in df_after.index],\"Outlier?\"]=\"Yes\"\n",
    "\n",
    "    import plotly.express as px\n",
    "    fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3',\n",
    "                  color='Outlier?',title=\"Visualization of Outliers using 3 Principle Components\")\n",
    "    \n",
    "    fig.show()\n",
    "    return df_pca\n",
    "find_PC = vizualize_outlier_detect(df_final_imputed,df_final_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove last outliers \n",
    "list_ID = find_PC[find_PC.PC2 > 5].index.to_list()\n",
    "vizualize_outlier_detect(df_final_imputed[~df_final_imputed.index.isin(list_ID)],df_final_imputed[~df_final_imputed.index.isin(list_ID)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b000c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "df_after_pca = pd.concat([df_final_imputed, pca_df], axis=1)\n",
    "df_after_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49da026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _color_red_or_green(val):\n",
    "    if val < -0.45:\n",
    "        color = 'background-color: red'\n",
    "    elif val > 0.45:\n",
    "        color = 'background-color: green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return color\n",
    "\n",
    "# Interpreting each Principal Component\n",
    "loadings = df_after_pca[metric_cols + pca_feat_names].corr().loc[metric_cols, pca_feat_names]\n",
    "loadings.style.applymap(_color_red_or_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e428c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(df,metric_columns,further_cols):\n",
    "    df.corr()\n",
    "    plt.figure(figsize=(15,15))\n",
    "    corr = df[metric_columns + further_cols].corr()\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(20, 220, n=200),\n",
    "        square=True, annot=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right'\n",
    "    )\n",
    "    ax.set_yticklabels(\n",
    "        ax.get_yticklabels(),\n",
    "        rotation=45,\n",
    "    );\n",
    "    \n",
    "corr_heatmap(df_after_pca,metric_cols,pca_feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb954a",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we mention in 3.4.2 we engineered \n",
    "Prem = ['PremMotor', 'PremHousehold',\n",
    "       'PremHealth', 'PremLife', 'PremWork']\n",
    "df_final_imputed.loc[:,\"Prem/Income\"]=df_final_imputed[Prem].sum(axis=1) / df_final_imputed.MonthSal\n",
    "df_final_imputed.loc[:,\"Profitable\"]= np.where(df_final_imputed.CustMonVal>0,1,0)\n",
    "df_final_imputed.loc[:,\"Amount_paid_by_Insur_last_2_years\"]= df_final_imputed.ClaimsRate * 2 * df_final_imputed[Prem].sum(axis=1)\n",
    "#file.loc[:,\"Age_first_purchase\"]= file.FirstPolYear - file.BirthYear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a43ef0",
   "metadata": {},
   "source": [
    "### Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf07d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(df,metric_columns,further_cols):\n",
    "    df.corr()\n",
    "    plt.figure(figsize=(15,15))\n",
    "    corr = df[metric_columns + further_cols].corr()\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(20, 220, n=200),\n",
    "        square=True, annot=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right'\n",
    "    )\n",
    "    ax.set_yticklabels(\n",
    "        ax.get_yticklabels(),\n",
    "        rotation=45,\n",
    "    );\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "corr_heatmap(df_final_imputed,metric_cols,[\"Prem/Income\",\"Profitable\",\"Amount_paid_by_Insur_last_2_years\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will later use these functions to scale our data for clustering and evaluation\n",
    "def Scale_Data_MINMAX(df):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # transform data\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale_Data_SS(df):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # transform data\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as explained in report 3.4.3 we removed features based on their correlation to each other and based on their own variance\n",
    "scaled_imputed = Scale_Data_MINMAX(df_final_imputed)\n",
    "secound_stage = scaled_imputed.describe().T[[\"std\"]]\n",
    "secound_stage[\"var\"] = secound_stage**2\n",
    "secound_stage.sort_values(\"var\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eac225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of final Feature Selection\n",
    "af_post_feat_sel = df_final_imputed.drop([\"MonthSal\",\"PremMotor\",\"ClaimsRate\",\"Prem/Income\",\"Amount_paid_by_Insur_last_2_years\"],1)\n",
    "\n",
    "\n",
    "#metric_cols.append(\"MonthSal\")\n",
    "#metric_cols.append(\"ClaimsRate\")\n",
    "#metric_cols.append(\"PremMotor\")\n",
    "#metric_cols.append(\"Prem/Income\")\n",
    "\n",
    "\n",
    "metric_cols.remove(\"MonthSal\")\n",
    "metric_cols.remove(\"ClaimsRate\")\n",
    "metric_cols.remove(\"PremMotor\")\n",
    "\n",
    "nonmetric_cols.append(\"Profitable\")\n",
    "\n",
    "metric_cols_new = metric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8acf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Correlation Heatmap\n",
    "corr_heatmap(df_final_imputed,metric_cols,[\"Prem/Income\",\"Profitable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We found this code in the internet its based on the Paper \"Spectral feature selection for supervised and unsupervised learning\"\n",
    "# But we didnt agreed with all results of it, therefor we didnt really used it and didnt mentioned it in our report \n",
    "import numpy.matlib\n",
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "def spec(X, **kwargs):\n",
    "    \"\"\"\n",
    "    This function implements the SPEC feature selection\n",
    "    Input\n",
    "    -----\n",
    "    X: {numpy array}, shape (n_samples, n_features)\n",
    "        input data\n",
    "    kwargs: {dictionary}\n",
    "        style: {int}\n",
    "            style == -1, the first feature ranking function, use all eigenvalues\n",
    "            style == 0, the second feature ranking function, use all except the 1st eigenvalue\n",
    "            style >= 2, the third feature ranking function, use the first k except 1st eigenvalue\n",
    "        W: {sparse matrix}, shape (n_samples, n_samples}\n",
    "            input affinity matrix\n",
    "    Output\n",
    "    ------\n",
    "    w_fea: {numpy array}, shape (n_features,)\n",
    "        SPEC feature score for each feature\n",
    "    Reference\n",
    "    ---------\n",
    "    Zhao, Zheng and Liu, Huan. \"Spectral Feature Selection for Supervised and Unsupervised Learning.\" ICML 2007.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'style' not in kwargs:\n",
    "        kwargs['style'] = 0\n",
    "    if 'W' not in kwargs:\n",
    "        kwargs['W'] = rbf_kernel(X, gamma=1)\n",
    "\n",
    "    style = kwargs['style']\n",
    "    W = kwargs['W']\n",
    "    if type(W) is numpy.ndarray:\n",
    "        W = csc_matrix(W)\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # build the degree matrix\n",
    "    X_sum = np.array(W.sum(axis=1))\n",
    "    D = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        D[i, i] = X_sum[i]\n",
    "\n",
    "    # build the laplacian matrix\n",
    "    L = D - W\n",
    "    d1 = np.power(np.array(W.sum(axis=1)), -0.5)\n",
    "    d1[np.isinf(d1)] = 0\n",
    "    d2 = np.power(np.array(W.sum(axis=1)), 0.5)\n",
    "    v = np.dot(np.diag(d2[:, 0]), np.ones(n_samples))\n",
    "    v = v/LA.norm(v)\n",
    "\n",
    "    # build the normalized laplacian matrix\n",
    "    L_hat = (np.matlib.repmat(d1, 1, n_samples)) * np.array(L) * np.matlib.repmat(np.transpose(d1), n_samples, 1)\n",
    "\n",
    "    # calculate and construct spectral information\n",
    "    s, U = np.linalg.eigh(L_hat)\n",
    "    s = np.flipud(s)\n",
    "    U = np.fliplr(U)\n",
    "\n",
    "    # begin to select features\n",
    "    w_fea = np.ones(n_features)*1000\n",
    "\n",
    "    for i in range(n_features):\n",
    "        f = X[:, i]\n",
    "        F_hat = np.dot(np.diag(d2[:, 0]), f)\n",
    "        l = LA.norm(F_hat)\n",
    "        if l < 100*np.spacing(1):\n",
    "            w_fea[i] = 1000\n",
    "            continue\n",
    "        else:\n",
    "            F_hat = F_hat/l\n",
    "        a = np.array(np.dot(np.transpose(F_hat), U))\n",
    "        a = np.multiply(a, a)\n",
    "        a = np.transpose(a)\n",
    "\n",
    "        # use f'Lf formulation\n",
    "        if style == -1:\n",
    "            w_fea[i] = np.sum(a * s)\n",
    "        # using all eigenvalues except the 1st\n",
    "        elif style == 0:\n",
    "            a1 = a[0:n_samples-1]\n",
    "            w_fea[i] = np.sum(a1 * s[0:n_samples-1])/(1-np.power(np.dot(np.transpose(F_hat), v), 2))\n",
    "        # use first k except the 1st\n",
    "        else:\n",
    "            a1 = a[n_samples-style:n_samples-1]\n",
    "            w_fea[i] = np.sum(a1 * (2-s[n_samples-style: n_samples-1]))\n",
    "\n",
    "    if style != -1 and style != 0:\n",
    "        w_fea[w_fea == 1000] = -1000\n",
    "\n",
    "    return w_fea\n",
    "\n",
    "\n",
    "def feature_ranking(score, **kwargs):\n",
    "    if 'style' not in kwargs:\n",
    "        kwargs['style'] = 0\n",
    "    style = kwargs['style']\n",
    "\n",
    "    # if style = -1 or 0, ranking features in descending order, the higher the score, the more important the feature is\n",
    "    if style == -1 or style == 0:\n",
    "        idx = np.argsort(score, 0)\n",
    "        return idx[::-1]\n",
    "    # if style != -1 and 0, ranking features in ascending order, the lower the score, the more important the feature is\n",
    "    elif style != -1 and style != 0:\n",
    "        idx = np.argsort(score, 0)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = spec(df_final_imputed.values)\n",
    "print(\"The higher the score, the more important the feature is\")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd06221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19185b",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6b05b",
   "metadata": {},
   "source": [
    "#### This Dataset is the final preprocessed Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset including Feature selection\n",
    "#af_post_feat_sel = pd.read_csv(\"data_ready_for_clustering.csv\")\n",
    "af_post_feat_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf14e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining metric_cols\n",
    "metric_cols = ['BirthYear',\n",
    " 'CustMonVal',\n",
    " 'PremHousehold',\n",
    " 'PremHealth',\n",
    " 'PremLife',\n",
    " 'PremWork']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa49c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining nonmetric_cols\n",
    "\n",
    "nonmetric_cols = ['EducDeg', 'GeoLivArea', 'Children', 'Profitable']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be20339",
   "metadata": {},
   "source": [
    "## Stage One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845935c",
   "metadata": {},
   "source": [
    "#### Visualization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5017f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I mentioned in the 4 captor of our report, its what we developed for the first stage evaluation of the models\n",
    "# the code is pretty generic, we mostly scaled, fitted and transformed with TSNE, PCA or UMAP and then plotted it with plotly\n",
    "# Visualization with PCA\n",
    "def vizualize_cluster(df_with_cluster,metric_columns, nonmetic_columns,cluster_column):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    df= df_with_cluster[metric_columns + nonmetic_columns].copy()\n",
    "    \n",
    "    standard_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_columns]),columns=metric_columns,index=df[metric_columns].index)\n",
    "    df_pca=pd.DataFrame(pca.fit_transform(standard_scaled_df),columns=[\"PC1\",\"PC2\",\"PC3\"],index=df[metric_columns].index)\n",
    "    df_pca[\"cluster\"] = df_with_cluster[cluster_column]\n",
    "\n",
    "    import plotly.express as px\n",
    "    fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3',\n",
    "                  color=cluster_column,title=\"Visualization of Outliers using 3 Principle Components\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "#Visualization with TSNE (unscaled)\n",
    "def TSNE_Visualization(df,metric_features, cluster_col):\n",
    "    from sklearn.manifold import TSNE\n",
    "    import plotly.express as px\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # This is step can be quite time consuming\n",
    "    TSNE1 = TSNE(random_state=38,n_components=2,perplexity = 18)\n",
    "    scaler = MinMaxScaler()\n",
    "    MinMax_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    TSNE1 = TSNE1.fit_transform(MinMax_scaled_df[metric_features])\n",
    "    some_df = df[[str(cluster_col)]].astype(int)\n",
    "\n",
    "    fig = px.scatter(\n",
    "    TSNE1, x=0, y=1,\n",
    "    color = some_df[str(cluster_col)].astype(\"str\"),width=900,\n",
    "    height=900,labels={'color': 'cluster'},title=\"Dimensionreduction 2D with TSNE and MinMaxScaler\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "#Visualization with TSNE (unscaled)\n",
    "def TSNE_Visualization_scaled(df,metric_features, cluster_col):\n",
    "    from sklearn.manifold import TSNE\n",
    "    import plotly.express as px\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # This is step can be quite time consuming\n",
    "    TSNE1 = TSNE(random_state=38,n_components=3,perplexity = 18)\n",
    "    scaler = MinMaxScaler()\n",
    "    min_max_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    TSNE1 = TSNE1.fit_transform(min_max_scaled_df[metric_features])\n",
    "    some_df = df[[str(cluster_col)]].astype(int)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        TSNE1, x=0, y=1, z=2,\n",
    "        color=some_df[str(cluster_col)],width=900,\n",
    "        height=900,title=\"Dimensionreduction 3D with TSNE and MinMaxScaler\",labels={'color': 'cluster'},)\n",
    "    fig.update_traces(marker=dict(size=5,\n",
    "                              line=dict(width=2,\n",
    "                                        color='Black')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# UMAD Visualization (unscaled)    \n",
    "def UMAD_Visualization(df,metric_features, cluster_col):\n",
    "    import umap.umap_ as umap\n",
    "    import plotly.express as px\n",
    "    # This is step can be quite time consuming\n",
    "    UMAP1 = umap.UMAP(n_components=2, init='random', random_state=38)\n",
    "    scaler = MinMaxScaler()\n",
    "    min_max_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    UMAP1 = UMAP1.fit_transform(min_max_scaled_df[metric_features])\n",
    "    some_df = df[[str(cluster_col)]].astype(int)\n",
    "\n",
    "    fig = px.scatter(\n",
    "    UMAP1, x=0, y=1,\n",
    "    color = some_df[str(cluster_col)].astype(\"str\"),width=900,\n",
    "    height=900,labels={'color': 'cluster'},title=\"Dimensionreduction 2D with UMAD and MinMaxScaler\")\n",
    "    \n",
    "    fig.show()    \n",
    "\n",
    "    \n",
    "# UMAD Visualization (Standardscaled)\n",
    "def UMAD_Visualization_StandardScaled(df,metric_features, cluster_col):\n",
    "    import umap.umap_ as umap\n",
    "    import plotly.express as px\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # This is step can be quite time consuming\n",
    "    UMAP1 = umap.UMAP(n_components=3, init='random', random_state=38)\n",
    "    scaler = StandardScaler()\n",
    "    standard_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    UMAP1 = UMAP1.fit_transform(standard_scaled_df[metric_features])\n",
    "    some_df = df[[str(cluster_col)]].astype(int)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        UMAP1, x=0, y=1, z=2,\n",
    "        color=some_df[str(cluster_col)].astype(\"str\"),width=900,\n",
    "        height=900,labels={'color': 'cluster'},title=\"Dimensionreduction with UMAD and StandardScaler\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# UMAD Visualization (MinMaxscaled)\n",
    "def UMAD_Visualization_MinMaxScaled(df,metric_features, cluster_col):\n",
    "    import umap.umap_ as umap\n",
    "    import plotly.express as px\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # This is step can be quite time consuming\n",
    "    UMAP1 = umap.UMAP(n_components=3, init='random', random_state=38)\n",
    "    scaler = MinMaxScaler()\n",
    "    min_max_scaled_df = pd.DataFrame(scaler.fit_transform(df[metric_cols]),columns=metric_cols,index=df[metric_cols].index)\n",
    "    UMAP1 = UMAP1.fit_transform(min_max_scaled_df[metric_features])\n",
    "    some_df = df[[str(cluster_col)]].astype(int)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        UMAP1, x=0, y=1, z=2,\n",
    "        color=some_df[str(cluster_col)].astype(\"str\"),width=900,\n",
    "        height=900,labels={'color': 'cluster'},title=\"Dimensionreduction with UMAD and MinMaxScaler\")\n",
    "    fig.update_traces(marker=dict(size=5,\n",
    "                              line=dict(width=2,\n",
    "                                        color='Black')),\n",
    "                  selector=dict(mode='markers'))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf63794",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# related to 4.1 First Stage our Report\n",
    "visual_df = af_post_feat_sel.copy()\n",
    "visual_df[\"cluster\"] = [2 for x in range(len(visual_df))]\n",
    "UMAD_Visualization(visual_df,metric_cols, \"cluster\")\n",
    "TSNE_Visualization(visual_df,metric_cols, \"cluster\")\n",
    "\n",
    "TSNE_Visualization_scaled(visual_df,metric_cols, \"cluster\")\n",
    "UMAD_Visualization_MinMaxScaled(visual_df,metric_cols, \"cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591b90e",
   "metadata": {},
   "source": [
    "### Silhouette and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this later to see the best number of clusters for different algorithms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import rcParams\n",
    "\n",
    "def SelBest(arr:list, X:int)->list:\n",
    "    '''\n",
    "    returns the set of X configurations with shorter distance\n",
    "    '''\n",
    "    dx=np.argsort(arr)[:X]\n",
    "    return arr[dx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00751e77",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ac78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "X = Scale_Data_MINMAX(af_post_feat_sel[metric_cols])\n",
    "\n",
    "linkage = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "for method in linkage:\n",
    "\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    model = AgglomerativeClustering(distance_threshold=0, n_clusters=None,linkage = method)\n",
    "\n",
    "    model = model.fit(X)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(f\"Hierarchical Clustering Dendrogram:{method} linkage\")\n",
    "    # plot the top six levels of the dendrogram\n",
    "    plot_dendrogram(model, truncate_mode=\"level\", p=3)\n",
    "    #plt.hlines(6.3, 0, 2000, colors=\"r\", linestyles=\"dashed\")\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.show()\n",
    "    print(\"____________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: Practical Class\n",
    "def get_r2_hc(df, link_method, max_nclus, min_nclus=1, dist=\"euclidean\"):\n",
    "    \"\"\"This function computes the R2 for a set of cluster solutions given by the application of a hierarchical method.\n",
    "    The R2 is a measure of the homogenity of a cluster solution. It is based on SSt = SSw + SSb and R2 = SSb/SSt. \n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to apply clustering\n",
    "    link_method (str): either \"ward\", \"complete\", \"average\", \"single\"\n",
    "    max_nclus (int): maximum number of clusters to compare the methods\n",
    "    min_nclus (int): minimum number of clusters to compare the methods. Defaults to 1.\n",
    "    dist (str): distance to use to compute the clustering solution. Must be a valid distance. Defaults to \"euclidean\".\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: R2 values for the range of cluster solutions\n",
    "    \"\"\"\n",
    "    def get_ss(df):\n",
    "        ss = np.sum(df.var() * (df.count() - 1))\n",
    "        return ss  # return sum of sum of squares of each df variable\n",
    "    \n",
    "    sst = get_ss(df)  # get total sum of squares\n",
    "    \n",
    "    r2 = []  # where we will store the R2 metrics for each cluster solution\n",
    "    \n",
    "    for i in tqdm(range(min_nclus, max_nclus+1)):  # iterate over desired ncluster range\n",
    "        cluster = AgglomerativeClustering(n_clusters=i, affinity=dist, linkage=link_method)\n",
    "        \n",
    "        \n",
    "        hclabels = cluster.fit_predict(df) #get cluster labels\n",
    "        \n",
    "        \n",
    "        df_concat = pd.concat((df, pd.Series(hclabels, name='labels')), axis=1)  # concat df with labels\n",
    "        \n",
    "        \n",
    "        ssw_labels = df_concat.groupby(by='labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "        \n",
    "        \n",
    "        ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "        \n",
    "        \n",
    "        r2.append(ssb / sst)  # save the R2 of the given cluster solution\n",
    "        \n",
    "    return np.array(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34307355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input\n",
    "hc_methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "# Call function defined above to obtain the R2 statistic for each hc_method\n",
    "max_nclus = 11\n",
    "r2_hc_methods = np.vstack(\n",
    "    [\n",
    "        get_r2_hc(df=X[metric_cols], link_method=link, max_nclus=max_nclus) \n",
    "        for link in hc_methods\n",
    "    ]\n",
    ").T\n",
    "r2_hc_methods = pd.DataFrame(r2_hc_methods, index=range(1, max_nclus + 1), columns=hc_methods)\n",
    "\n",
    "sns.set()\n",
    "# Plot data\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "sns.lineplot(data=r2_hc_methods, linewidth=2.5, markers=[\"o\"]*4)\n",
    "\n",
    "# Finalize the plot\n",
    "fig.suptitle(\"R2 plot for various hierarchical methods\", fontsize=21)\n",
    "plt.gca().invert_xaxis()  # invert x axis\n",
    "plt.legend(title=\"HC methods\", title_fontsize=11)\n",
    "plt.xticks(range(1, max_nclus + 1))\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8dfd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final hierachical clustering \n",
    "df_AGGLO = af_post_feat_sel.copy()\n",
    "agglomerative_c = AgglomerativeClustering(n_clusters=3,linkage = \"ward\")\n",
    "labels_agglomerative = agglomerative_c.fit_predict(MinMaxScaler().fit_transform(df_AGGLO[metric_cols]))\n",
    "df_AGGLO.loc[:,\"cluster\"] = labels_agglomerative\n",
    "#UMAD_Visualization_StandardScaled(df_AGGLO,metric_cols,\"cluster\")\n",
    "#Visualization of our results\n",
    "TSNE_Visualization_scaled(df_AGGLO,metric_cols,\"cluster\")\n",
    "UMAD_Visualization_MinMaxScaled(df_AGGLO,metric_cols,\"cluster\")\n",
    "df_AGGLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e79f45",
   "metadata": {},
   "source": [
    "### GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551f7d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "#using this to get the best nr of clusters\n",
    "# Storing average silhouette metric\n",
    "df = Scale_Data_MINMAX(af_post_feat_sel).copy()\n",
    "metric_features = metric_cols\n",
    "range_clusters = range(1, 11)\n",
    "\n",
    "avg_silhouette = []\n",
    "for nclus in range_clusters:\n",
    "    # Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    # Initialize the KMeans object with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    GMMclust = GaussianMixture(n_components=nclus,tol=0.5)\n",
    "    cluster_labels = GMMclust.fit_predict(df[metric_features])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the densyity and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(df[metric_features], cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df[metric_features], cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        # Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    # The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(df[metric_features]) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no clue where I found this code, but its not mine ;-)\n",
    "# anyways here we just visualize the optimal number of clusters using GaussianMixture as Model\n",
    "df_copy = af_post_feat_sel.copy()\n",
    "n_clusters=np.arange(2, 25)\n",
    "sils=[]\n",
    "sils_err=[]\n",
    "iterations=8\n",
    "for n in n_clusters:\n",
    "    tmp_sil=[]\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        gmm=GaussianMixture(n_components=n).fit(Scale_Data_MINMAX(df_copy)[metric_cols]) \n",
    "        labels=gmm.predict(Scale_Data_MINMAX(df_copy)[metric_cols])\n",
    "        sil=metrics.silhouette_score(Scale_Data_MINMAX(df_copy)[metric_cols], labels, metric='euclidean')\n",
    "        tmp_sil.append(sil)\n",
    "    val=np.mean(SelBest(np.array(tmp_sil), int(iterations/5)))\n",
    "    err=np.std(tmp_sil)\n",
    "    sils.append(val)\n",
    "    sils_err.append(err)\n",
    "\n",
    "plt.errorbar(n_clusters, sils, yerr=sils_err)\n",
    "plt.title(\"Silhouette Scores\", fontsize=20)\n",
    "plt.xticks(n_clusters)\n",
    "plt.xlabel(\"N. of clusters\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0abd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of the search for the best hyperparameter, we did the same with TOL and Reg_covar\n",
    "df_copy = af_post_feat_sel.copy()\n",
    "some_dict = dict()\n",
    "for _ in tqdm(range(180,200)):\n",
    "    gmm=GaussianMixture(n_components=3,random_state = _,tol = 0.5,reg_covar = 1e-5,max_iter = 400,init_params = \"random\").fit(Scale_Data_MINMAX(df_copy)[metric_cols]) \n",
    "    labels=gmm.predict(Scale_Data_MINMAX(df_copy)[metric_cols])\n",
    "    sil=metrics.silhouette_score(Scale_Data_MINMAX(df_copy)[metric_cols], labels, metric='euclidean')\n",
    "    some_dict[_] = sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f256fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "gmm=GaussianMixture(n_components=3,random_state = 179,tol = 1).fit(Scale_Data_MINMAX(df_copy)[metric_cols]) \n",
    "labels=gmm.predict(Scale_Data_MINMAX(df_copy)[metric_cols])\n",
    "sil=metrics.silhouette_score(Scale_Data_MINMAX(df_copy)[metric_cols], labels, metric='euclidean')\n",
    "sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "gmm=GaussianMixture(n_components=3,random_state = 186,tol = 0.1,reg_covar = 1e-5,max_iter = 400).fit(Scale_Data_MINMAX(df_copy)[metric_cols]) \n",
    "labels=gmm.predict(Scale_Data_MINMAX(df_copy)[metric_cols])\n",
    "sil=metrics.silhouette_score(Scale_Data_MINMAX(df_copy)[metric_cols], labels, metric='euclidean')\n",
    "sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result of Loop above\n",
    "max(some_dict,key = some_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae020f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training gaussian mixture model \n",
    "# final solution and visualization\n",
    "from sklearn.mixture import GaussianMixture\n",
    "df_GMM = af_post_feat_sel.copy()\n",
    "\n",
    "GMM = GaussianMixture(n_components=3,random_state = 179,tol=0.5,reg_covar = 1e-5)\n",
    "\n",
    "#predictions from gmm\n",
    "\n",
    "labels_GMM = GMM.fit_predict(Scale_Data_MINMAX(df_GMM)[metric_cols])\n",
    "df_GMM.loc[:,\"cluster\"] = labels_GMM\n",
    "TSNE_Visualization_scaled(df_GMM,metric_cols,\"cluster\")\n",
    "UMAD_Visualization_MinMaxScaled(df_GMM,metric_cols,\"cluster\")\n",
    "vizualize_cluster(df_GMM,metric_cols, nonmetric_cols,\"cluster\")\n",
    "df_GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secound stage evaluation of different models\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "metrices= [\"Silhouette(max)\",\"Calinski-Harabasz(max)\",\"Davies-Bouldin(min)\"]\n",
    "clustmethod= [\"KMeans\",\"DBScan\",\"Gaussian Mixture\"]\n",
    "evalu = pd.DataFrame(np.zeros((len(metrices),len(clustmethod))),index=metrices,columns=clustmethod)\n",
    "\n",
    "def evaluation(df,cluster_label,clmethod_name):\n",
    "    evalu.loc[metrices,clmethod_name]=[silhouette_score(df,cluster_label)\n",
    "                                       ,calinski_harabasz_score(df, cluster_label)\n",
    "                                       ,davies_bouldin_score(df,cluster_label)]\n",
    "    \n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_GMM,\"Gaussian Mixture_3_tol_scaled_1e-5_final\")\n",
    "evalu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808c473",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22250566",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "# same we did for GMM we do it for spectral as well - results are more or less the same\n",
    "# Storing average silhouette metric\n",
    "df = Scale_Data_MINMAX(af_post_feat_sel).copy()\n",
    "metric_features = metric_cols\n",
    "range_clusters = range(1, 11)\n",
    "\n",
    "avg_silhouette = []\n",
    "for nclus in range_clusters:\n",
    "    # Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    # Initialize the KMeans object with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    SPECclust = SpectralClustering(n_components=nclus,assign_labels='discretize')\n",
    "    cluster_labels = SPECclust.fit_predict(df[metric_features])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(df[metric_features], cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df[metric_features], cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        # Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    # The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(df[metric_features]) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71839476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SPEC = af_post_feat_sel.copy()\n",
    "n_clusters=np.arange(2, 7)\n",
    "sils=[]\n",
    "sils_err=[]\n",
    "iterations=8\n",
    "for n in n_clusters:\n",
    "    tmp_sil=[]\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        SPEC = SpectralClustering(n_components=n,assign_labels='discretize')\n",
    "        labels=SPEC.fit_predict(Scale_Data_MINMAX(df_SPEC)[metric_cols])\n",
    "        sil=metrics.silhouette_score(Scale_Data_MINMAX(df_SPEC)[metric_cols], labels, metric='euclidean')\n",
    "        tmp_sil.append(sil)\n",
    "    val=np.mean(SelBest(np.array(tmp_sil), int(iterations/5)))\n",
    "    err=np.std(tmp_sil)\n",
    "    sils.append(val)\n",
    "    sils_err.append(err)\n",
    "plt.errorbar(n_clusters, sils, yerr=sils_err)\n",
    "plt.title(\"Silhouette Scores\", fontsize=20)\n",
    "plt.xticks(n_clusters)\n",
    "plt.xlabel(\"N. of clusters\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2eb61e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#! pip install pyamg\n",
    "# training spectral model \n",
    "from sklearn.cluster import SpectralClustering\n",
    "df_SPEC = af_post_feat_sel.copy()\n",
    "SPEC = SpectralClustering(n_components=3,assign_labels = \"discretize\",eigen_solver = \"arpack\")\n",
    "\n",
    "#predictions from SPEC\n",
    "\n",
    "labels_SPEC = SPEC.fit_predict(Scale_Data_MINMAX(df_SPEC)[metric_cols])\n",
    "df_SPEC.loc[:,\"cluster\"] = labels_SPEC\n",
    "UMAD_Visualization_MinMaxScaled(df_SPEC,metric_cols,\"cluster\")\n",
    "TSNE_Visualization_scaled(df_SPEC,metric_cols,\"cluster\")\n",
    "vizualize_cluster(df_SPEC,metric_cols, nonmetric_cols,\"cluster\")\n",
    "df_SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537bc77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training spectral model \n",
    "from sklearn.cluster import SpectralClustering\n",
    "df_SPEC2 = af_post_feat_sel.copy()\n",
    "SPEC = SpectralClustering(n_components=3,assign_labels='discretize')\n",
    "\n",
    "#predictions from SPEC\n",
    "\n",
    "labels_SPEC = SPEC.fit_predict(Scale_Data_MINMAX(df_SPEC2)[metric_cols])\n",
    "df_SPEC2.loc[:,\"cluster\"] = labels_SPEC\n",
    "UMAD_Visualization_StandardScaled(df_SPEC2,metric_cols,\"cluster\")\n",
    "UMAD_Visualization_MinMaxScaled(df_SPEC2,metric_cols,\"cluster\")\n",
    "vizualize_cluster(df_SPEC2,metric_cols, nonmetric_cols,\"cluster\")\n",
    "df_SPEC2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303c8e2",
   "metadata": {},
   "source": [
    "### OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df638e33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training Optics\n",
    "from sklearn.cluster import OPTICS\n",
    "df_OPT = af_post_feat_sel.copy()\n",
    "OPT = OPTICS(min_samples=4,eps = 0.0001,metric = \"euclidean\")\n",
    "\n",
    "#predictions from OPT\n",
    "\n",
    "labels_OPT = OPT.fit_predict(Scale_Data_MINMAX(df_OPT[metric_cols]))\n",
    "df_OPT.loc[:,\"cluster\"] = labels_OPT\n",
    "UMAD_Visualization_StandardScaled(df_OPT,metric_cols,\"cluster\")\n",
    "UMAD_Visualization_MinMaxScaled(df_OPT,metric_cols,\"cluster\")\n",
    "df_OPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eab0ec",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bb183",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "# Storing average silhouette metric\n",
    "df = af_post_feat_sel.copy()\n",
    "metric_features = metric_cols\n",
    "range_clusters = range(1, 11)\n",
    "\n",
    "avg_silhouette = []\n",
    "for nclus in range_clusters:\n",
    "    # Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    # Initialize the KMeans object with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=15, random_state=1)\n",
    "    cluster_labels = kmclust.fit_predict(df[metric_features])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(df[metric_features], cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df[metric_features], cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        # Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    # The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(df[metric_features]) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681bdbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "df_KMEANS = af_post_feat_sel.copy()\n",
    "KMEANS1 = KMeans(n_clusters=4,init = \"k-means++\" ,random_state=38,n_init = 100)\n",
    "\n",
    "\n",
    "# training and prediction KMEANS\n",
    "\n",
    "labels_KMEANS = KMEANS1.fit_predict(Scale_Data_MINMAX(df_KMEANS))\n",
    "df_KMEANS.loc[:,\"cluster\"] = labels_KMEANS\n",
    "\n",
    "UMAD_Visualization_StandardScaled(df_KMEANS,metric_cols,\"cluster\")\n",
    "UMAD_Visualization_MinMaxScaled(df_KMEANS,metric_cols,\"cluster\")\n",
    "vizualize_cluster(df_KMEANS,metric_cols, nonmetric_cols,\"cluster\")\n",
    "df_KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6560751",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd39b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#For hyperparameter min_sample we choose the rule of thumb (number of features -1) = 5.\n",
    "#Tune hypterparameter epsilon for dbscan: \n",
    "df_DB = af_post_feat_sel.copy()\n",
    "\n",
    "for epsil in tqdm(np.arange(0.001, 0.3, 0.001)):\n",
    "\n",
    "    db = DBSCAN(\n",
    "      eps = epsil,\n",
    "      metric=\"euclidean\",\n",
    "      min_samples = 5,\n",
    "      n_jobs = -1)\n",
    "    clusters = db.fit_predict(Scale_Data_MINMAX(df_DB)[metric_cols])\n",
    "\n",
    "\n",
    "\n",
    "    unique, counts = np.unique(clusters, return_counts=True)\n",
    "    dbscan = dict(zip(unique, counts))\n",
    "\n",
    "    for key,value in dbscan.copy().items():\n",
    "\n",
    "        #We want to ignore tiny clusters with less than 40 data points and outliers with the label -1.\n",
    "        if (value < 40) or (key == -1):\n",
    "            del dbscan[key]\n",
    "    #Only consider epsilons that result in at least 3 clusters of significant size.\n",
    "    if len(dbscan) > 2:\n",
    "        print(epsil,dbscan)\n",
    "        \n",
    "#We choose an epsilon of 0.056 as we get at least 2 clusters of signifcant size\n",
    "\n",
    "db = DBSCAN(\n",
    "      eps = 0.056,\n",
    "      metric=\"euclidean\",\n",
    "      min_samples = 5,\n",
    "      n_jobs = -1)\n",
    "labels_dbscan = db.fit_predict(Scale_Data_MINMAX(df_DB)[metric_cols])   \n",
    "df_DB.loc[:,\"cluster\"] = labels_dbscan\n",
    "UMAD_Visualization_MinMaxScaled(df_DB,metric_cols,\"cluster\")\n",
    "\n",
    "#DB Scan is working really bad for our dataset. Reason for that is that the point density is not decreasing between the clusters. Clusters in our dataset are by no means center of high density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b6510",
   "metadata": {},
   "source": [
    "### Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e263a66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "df_BRCH = af_post_feat_sel.copy()\n",
    "BRCH = Birch(n_clusters=3,branching_factor =150)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training and prediction BIRCH\n",
    "\n",
    "labels_BRCH = BRCH.fit_predict(Scale_Data_MINMAX(df_KMEANS))\n",
    "df_BRCH.loc[:,\"cluster\"] = labels_BRCH\n",
    "\n",
    "UMAD_Visualization_StandardScaled(df_BRCH,metric_cols,\"cluster\")\n",
    "UMAD_Visualization_MinMaxScaled(df_BRCH,metric_cols,\"cluster\")\n",
    "vizualize_cluster(df_BRCH,metric_cols, nonmetric_cols,\"cluster\")\n",
    "df_BRCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4d562",
   "metadata": {},
   "source": [
    "### Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "metrices= [\"Silhouette(max)\",\"Calinski-Harabasz(max)\",\"Davies-Bouldin(min)\"]\n",
    "clustmethod= [\"KMeans\",\"DBScan\",\"Gaussian Mixture\"]\n",
    "evalu = pd.DataFrame(np.zeros((len(metrices),len(clustmethod))),index=metrices,columns=clustmethod)\n",
    "# Compute three evalluation metrices for the cluster labels \n",
    "def evaluation(df,cluster_label,clmethod_name):\n",
    "    evalu.loc[metrices,clmethod_name]=[silhouette_score(df,cluster_label)\n",
    "                                       ,calinski_harabasz_score(df, cluster_label)\n",
    "                                       ,davies_bouldin_score(df,cluster_label)]\n",
    "\n",
    "# DF is MinMaxScaled\n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_agglomerative,\"Hierachical\")\n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_GMM,\"Gaussian Mixture\")\n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_SPEC,\"Spectral Clustering_3\")\n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_OPT,\"OPTICS\")\n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_KMEANS,\"KMeans\")\n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_dbscan,\"DBScan\")\n",
    "evaluation(Scale_Data_MINMAX(af_post_feat_sel)[metric_cols],labels_BRCH,\"BIRCH\")\n",
    "\n",
    "\n",
    "evalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360aa5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF the DF is standardscaled\n",
    "evaluation(Scale_Data_SS(af_post_feat_sel)[metric_cols],labels_agglomerative,\"Hierachical\")\n",
    "evaluation(Scale_Data_SS(af_post_feat_sel)[metric_cols],labels_GMM,\"Gaussian Mixture\")\n",
    "evaluation(Scale_Data_SS(af_post_feat_sel)[metric_cols],labels_SPEC,\"Spectral Clustering\")\n",
    "evaluation(Scale_Data_SS(af_post_feat_sel)[metric_cols],labels_OPT,\"OPTICS\")\n",
    "evaluation(Scale_Data_SS(af_post_feat_sel)[metric_cols],labels_KMEANS,\"KMeans\")\n",
    "evaluation(Scale_Data_SS(af_post_feat_sel)[metric_cols],labels_dbscan,\"DBScan\")\n",
    "evaluation(Scale_Data_SS(af_post_feat_sel)[metric_cols],labels_BRCH,\"BIRCH\")\n",
    "\n",
    "\n",
    "\n",
    "evalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=np.arange(2, 10)\n",
    "bics=[]\n",
    "bics_err=[]\n",
    "iterations=20\n",
    "for n in n_clusters:\n",
    "    tmp_bic=[]\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        gmm=GaussianMixture(n_components=n).fit(af_post_feat_sel) \n",
    "        \n",
    "        tmp_bic.append(gmm.bic(af_post_feat_sel))\n",
    "    val=np.mean(SelBest(np.array(tmp_bic), int(iterations/5)))\n",
    "    err=np.std(tmp_bic)\n",
    "    bics.append(val)\n",
    "    bics_err.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(n_clusters,bics, yerr=bics_err, label='BIC')\n",
    "plt.title(\"BIC Scores\", fontsize=20)\n",
    "plt.xticks(n_clusters)\n",
    "plt.xlabel(\"N. of clusters\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal NR of Cluster with ElbowMethod and K-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "\n",
    "range_n_clusters = [1, 2, 3, 4, 5, 6]\n",
    "avg_distance=[]\n",
    "for n_clusters in range(1,20):\n",
    "    clusterer = KMeans(n_clusters=int(n_clusters), random_state=42).fit(af_post_feat_sel)\n",
    "    avg_distance.append(clusterer.inertia_)\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1,20), avg_distance)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8e5b1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_SPEC = df_SPEC.set_index(\"CustID\")\n",
    "\n",
    "#Compute the feature means of each cluster to comparte the characteristics of each cluster. We want to include all features for that.\n",
    "df_SPEC_all = pd.merge(df_SPEC[[\"cluster\"]],file, left_index=True, right_index=True,how='left')\n",
    "df_SPEC_all.drop([\"Missing_Values\"],axis = 1,inplace = True)\n",
    "df_SPEC_all.groupby(\"cluster\").mean().sort_values(\"BirthYear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d95994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GMM.groupby(\"cluster\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_GMM = df_GMM.set_index(\"CustID\")\n",
    "df_GMM_all = pd.merge(df_GMM[[\"cluster\"]],file, left_index=True, right_index=True,how='left')\n",
    "df_GMM_all.drop([\"Missing_Values\"],axis = 1,inplace = True)\n",
    "df_GMM_all.groupby(\"cluster\").mean().sort_values(\"BirthYear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_AGGLO = df_AGGLO.set_index(\"CustID\")\n",
    "df_aggl_all = pd.merge(df_AGGLO[[\"cluster\"]],file, left_index=True, right_index=True,how='left')\n",
    "df_aggl_all.drop([\"Missing_Values\"],axis = 1,inplace = True)\n",
    "df_aggl_all.groupby(\"cluster\").mean().sort_values(\"BirthYear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa27e6",
   "metadata": {},
   "source": [
    "## Classification of Outliers using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now build a classifier using KNN to assign a cluster to each outlier detected. We find the best number of K, which is 32 in out case.\n",
    "\n",
    "X_pred = Scale_Data_MINMAX(outlier[metric_cols])\n",
    "y = df_SPEC_all[\"cluster\"].astype(\"int\")\n",
    "X = Scale_Data_MINMAX(df_final_imputed[metric_cols])\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores=list()\n",
    "for k in tqdm(range(2,40)):\n",
    "\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores.append(cross_val_score(neigh, X.values, y, cv=10).mean())\n",
    "best_k = scores.index(max(scores)) + 1\n",
    "print(f\"The best k from 1 to 40 is: {best_k}.\")\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=best_k).fit(X,y)\n",
    "#Using cross validation to avoid overfitting\n",
    "print(f\"The cross validation accuracy score is {cross_val_score(neigh, X.values, y, cv=10).mean()}.\")\n",
    "y_pred= neigh.predict(X_pred)\n",
    "\n",
    "outlier = file.loc[outlier.index,:].drop(columns=\"Missing_Values\")\n",
    "outlier.loc[:,\"cluster\"]= y_pred\n",
    "\n",
    "final_clustered_df = pd.concat([outlier,df_SPEC_all]) # Add the outliers along with their predictions back to the other data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbebf9c",
   "metadata": {
    "id": "zkSsmRnrsl5q"
   },
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/sevamoo/SOMPY.git\n",
    "\n",
    "import sompy\n",
    "from sompy.visualization.mapview import View2D\n",
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "from sompy.visualization.hitmap import HitMapView\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db3c5a",
   "metadata": {
    "id": "j4QYi-ZXJIuH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_scaled_som = Scale_Data_MINMAX(af_post_feat_sel[metric_cols]).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ebfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veHjd6pTJi7E",
    "outputId": "43a828ef-6281-45ac-cbd4-971b74621fe5"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sm = sompy.SOMFactory().build(\n",
    "    df_scaled_som.values, \n",
    "    mapsize=[10, 10],\n",
    "    initialization='random', \n",
    "    neighborhood='gaussian',\n",
    "    training='batch',\n",
    "    lattice='hexa',\n",
    "    component_names=metric\n",
    ")\n",
    "sm.train(n_job=4, verbose='info', train_rough_len=100, train_finetune_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e0bcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "zILfBrx7KfXs",
    "outputId": "0ca3580a-4a64-4ad0-9041-62518c599e42"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "view2D = View2D(12, 12, \"\", text_size=10)\n",
    "view2D.show(sm, col_sz=3, what='codebook')\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.suptitle(\"Component Planes\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24b3d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "8lDW6rr1KyhH",
    "outputId": "635feaad-f76a-4128-bad3-385519f16d3f"
   },
   "outputs": [],
   "source": [
    "# Here you have U-matrix\n",
    "u = sompy.umatrix.UMatrixView(9, 9, 'umatrix-Avg Distance from each Neuron to its Neighbor', show_axis=True, text_size=8, show_text=True)\n",
    "\n",
    "UMAT = u.show(\n",
    "    sm, \n",
    "    distance=2, \n",
    "    row_normalized=False,\n",
    "    show_data=True, \n",
    "    contour=True, # Visualize isomorphic curves\n",
    "    blob=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a3f64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "g0xenUE3LMoU",
    "outputId": "197ec76d-2175-48c6-8dd3-da584af40102"
   },
   "outputs": [],
   "source": [
    "vhts  = BmuHitsView(12,12,\"Hits Map - Number of Points that each Neuron represent\")\n",
    "vhts.show(sm, anotate=True, onlyzeros=False, labelsize=12, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df63a8",
   "metadata": {
    "id": "A4bIaiZgQY2C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
